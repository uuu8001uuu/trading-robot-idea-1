{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_1_0.ipynb","provenance":[{"file_id":"1L0P32Qd7Yt9WmHIos2fwBzBS7gPQFk7V","timestamp":1628659374482},{"file_id":"1lGCGxw7BCgdK6wvGzpZ7gsjt7b04NZb9","timestamp":1627452317580}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-odnbgFDtlzk"},"source":["import pandas as pd\n","from datetime import timedelta\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error\n","from matplotlib import pyplot as plt\n","from typing import List\n","import datetime\n","import os\n","from tensorflow import keras\n","from tensorflow.keras import callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CkjbwcrSVqa"},"source":["# function calculate the execution time\n","start_time = datetime.datetime.now()\n","def execution_time_ml():\n","  stop_time = datetime.datetime.now()\n","  temp = stop_time-start_time\n","  return temp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BA9C8fdGNMT1"},"source":["# pd.options.display.max_columns = None # show all columns when they are not visible\n","# pd.options.display.max_rows = None # show all rows when they are not visible"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwgRNovjNOE4"},"source":["# log into Google Disk for Google Colab/ if you use file on ssd on your computer, this code removes\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# set default folder for python\n","import sys\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCulyQyMrrZE"},"source":["df = pd.read_csv('data_set.csv', index_col='Date')\n","df = df.drop(['Close_W'], axis=1) # this column is used in the predict so it needs to be removed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFB6AfMAzkyO"},"source":["def split_data(batch_size):\n","  # split data into train and values\n","  train_size = int(df.shape[0] * 0.8)\n","\n","  train_df = df.iloc[:train_size]\n","  val_df = df.iloc[train_size + batch_size:] # create space in front of training and values so that batch_size does not go over the values\n","  return train_df, val_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47WtlS5Gdn9O"},"source":["# Parameters model ML\n","patience = 75\n","# min_delta = 0.001\n","shuffle = True\n","use_scaler = True\n","optimizer = 'adam'\n","loss = 'MeanAbsolutePercentageError'\n","metrics='mae'\n","# dropout = 0.3\n","# lstm = 200\n","return_sequences = False\n","activation = 'relu'\n","dense_output = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWumyS1P1WOU"},"source":["def creation_scaler(train_df):\n","  scaler = StandardScaler()\n","  scaler.fit(train_df[[\"Close\"]])\n","  return scaler\n","\n","def make_dataset(\n","    scaler,\n","    df,\n","    window_size,\n","    batch_size,\n","    use_scaler=use_scaler,\n","    shuffle=shuffle\n","    ):\n","  features = df[[\"Close\"]].iloc[:-window_size]\n","  if use_scaler:\n","    features = scaler.transform(features)\n","  data = np.array(features, dtype=np.float32)\n","  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n","      data=data,\n","      targets=df[\"Close\"].iloc[window_size:],\n","      sequence_length=window_size,\n","      sequence_stride=1,\n","      shuffle=shuffle,\n","      batch_size=batch_size)\n","  return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ydtm-GkZ7HRy"},"source":["# make compile and fit model ML\n","def compile_and_fit(model, train_ds, val_ds, num_epochs, callbacks):\n","  model.compile(\n","        optimizer=optimizer,\n","        loss=loss,\n","        metrics=metrics,\n","      )\n","  history = model.fit(\n","      train_ds,\n","      epochs=num_epochs,\n","      validation_data=val_ds,\n","      callbacks=callbacks,\n","      verbose=1\n","      )\n","\n","  return history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7XbkzUrABzO"},"source":["# make layers model ML\n","def lstm_model(lstm, dropout, dense, use_load_model, directory_model, len_train_df):\n","  if use_load_model == True: # you can use load model ML from file\n","    model = tf.keras.models.load_model(directory_model) # you can use checpoint model for example training_step_1/cp-0024.ckpt\n","  else:\n","    model = tf.keras.models.Sequential([\n","      tf.keras.layers.LSTM(lstm, return_sequences=return_sequences),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Dropout(dropout),\n","      tf.keras.layers.Dense(dense, activation=activation, input_shape=[len_train_df-2]), # (len(train_df.columns)-2) calculate amount columns in train data_set\n","      tf.keras.layers.Dropout(dropout),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Dense(dense, activation=activation),\n","      tf.keras.layers.Dropout(dropout),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Dense(dense, activation=activation),\n","      tf.keras.layers.Dropout(dropout),\n","      tf.keras.layers.BatchNormalization(),\n","      tf.keras.layers.Dense(dense_output),\n","    ])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6tnTGGxkaDX"},"source":["# function early stopping ML on overfitting\n","def early_stopping_ml(min_delta):\n","  early_stopping = callbacks.EarlyStopping(\n","    patience=patience,\n","    min_delta=min_delta,\n","    restore_best_weights=True,\n","  )\n","  return early_stopping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNsp_KYGpR2F"},"source":["# function saving ML results to a file\n","from csv import writer\n","import os\n","def append_list_as_row(file_name, list_of_elem, header):\n","    with open(file_name, 'a+', newline='') as write_obj:\n","        list_of_elem.to_csv(write_obj, mode='a', header=header)\n","        write_obj.close()\n","\n","def saving_results_ml(result, parametr_save_result, directory_result):\n","  if parametr_save_result == True:\n","    if os.path.isfile(directory_result) == True: # checking there is a file in the folder\n","      append_list_as_row(directory_result, result, header=False) # the parameters header enables or disables the output of the header in the file\n","    else:\n","      append_list_as_row(directory_result, result, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuTqNJpisOmL"},"source":["# function saving full model ML to a file\n","def saving_full_model(parametr_save_model, model_ml, directory_model):\n","  if parametr_save_model == True:\n","    model_ml.save(directory_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmJgFQ4-0uwr"},"source":["def predict_calculate(lstm,\n","                      dropout,\n","                      dense,\n","                      batch_size,\n","                      min_delta,\n","                      parametr_save_model,\n","                      parametr_save_result,\n","                      epochs,\n","                      checkpoint_use,\n","                      use_load_model,\n","                      directory_model,\n","                      directory_result,\n","                      ):\n","  train_df, val_df = split_data(batch_size) # split data on train_df and val_df\n","  len_train_df = len(train_df.columns) # calculate amount columns in train data_set\n","  \n","  model_ml = lstm_model(lstm, dropout, dense, use_load_model, directory_model, len_train_df)    \n","  window_size = batch_size + 2\n","  \n","  if checkpoint_use == True: # function use checpoint model (step by step) in training ML\n","    checkpoint_path = \"training_step_2/cp-{epoch:04d}.ckpt\"\n","    checkpoint_dir = os.path.dirname(checkpoint_path)\n","    callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path, \n","    verbose=1,\n","    )\n","  else:\n","    callback = early_stopping_ml(min_delta) # function early stopping ML on overfitting\n","\n","  scaler = creation_scaler(train_df)\n","  train_ds = make_dataset(scaler,\n","                          df=train_df,\n","                          window_size=window_size,\n","                          batch_size=batch_size,\n","                          use_scaler=use_scaler,\n","                          shuffle=shuffle,\n","                          )\n","  val_ds = make_dataset(scaler,\n","                        df=val_df,\n","                        window_size=window_size,\n","                        batch_size=batch_size,\n","                        use_scaler=use_scaler,\n","                        shuffle=shuffle,\n","                        )    \n","  history =  compile_and_fit(model_ml, train_ds, val_ds, num_epochs=epochs, callbacks=callback)\n","\n","  history_df = pd.DataFrame(history.history)\n","  history_df.loc[:, ['loss', 'val_loss']].plot()\n","  print()\n","  print(\"Minimum Loss: {:0.4f}\".format(history_df['loss'].min()))\n","  print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n","\n","  metrics_names = metrics\n","  metrics_names_val = 'val_' + metrics\n","  history_df.loc[:, [metrics_names, metrics_names_val]].plot() # select values from history columns\n","  print()\n","  print(\"Minimum metrics: {:0.4f}\".format(history_df[metrics_names].min()))\n","  print(\"Minimum Validation metrics: {:0.4f}\".format(history_df[metrics_names_val].min()))\n","\n","\n","\n","  # function execution_time_ml() calculate the execution time code\n","  temp = execution_time_ml()\n","  print('Time to complete (h:m):', ':'.join(str(temp).split(':')[:2])) # show time in format h:m\n","\n","  # DataFrame creation with evaluation of results ML\n","  result = pd.DataFrame({'Minimum_Loss': [history_df['loss'].min()],\n","    'Minimum_Validation_Loss': [history_df['val_loss'].min()],\n","    'Batch_size': [batch_size],\n","    'Window_size': [window_size],\n","    'Epochs': [epochs],\n","    'Patience': [patience],\n","    'Min_delta': [min_delta],                       \n","    'Shuffle': [shuffle],\n","    'Use_scaler': [use_scaler],\n","    'Optimizer': [optimizer],\n","    'Loss': [loss],\n","    'Dropout': [dropout],\n","    'LSTM': [lstm],\n","    'Return_sequences': [return_sequences],\n","    'Activation': [activation],\n","    'Dense': [dense],\n","    'Dense_output': [dense_output],\n","    'Time_to_complete': [temp],\n","    })\n","\n","  saving_results_ml(result, parametr_save_result, directory_result) # function saving ML results to a file\n","\n","  saving_full_model(parametr_save_model, model_ml, directory_model) # function saving full model ML to a file\n","  "],"execution_count":null,"outputs":[]}]}